{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "cyekb5eV-uiA",
        "outputId": "2e032255-9db8-45a3-9a3c-8f6d9831e5c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. \\nWhen a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. \\nThen the model does not categorize the data correctly, because of too many details and noise.\\nA solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \\nOverfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. \\nBecause of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. \\nThe overfitted model has low bias and high variance.The chances of occurrence of overfitting increase as much we provide training to our model. \\nIt means the more we train our model, the more chances of occurring the overfitted model.\\nOverfitting is the main problem that occurs in supervised learning.\\nReasons for Overfitting are as follows:\\n(i) High variance and low bias \\n(ii) The model is too complex\\n(iii) The size of the training data \\nTechniques to reduce overfitting:\\n(i) Increase training data.\\n(ii) Reduce model complexity.\\n(iii) Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\\n(iv) Ridge Regularization and Lasso Regularization\\n(v) Use dropout for neural networks to tackle overfitting.\\n\\nUnderfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, \\ni.e., it only performs well on training data but performs poorly on testing data. \\nUnderfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. \\nUnderfitting can be avoided by using more data and also reducing the features by feature selection. \\nReasons for Underfitting:\\n(i) High bias and low variance \\n(ii) The size of the training dataset used is not enough.\\n(iii) The model is too simple.\\n(iv) Training data is not cleaned and also contains noise in it.\\nTechniques to reduce underfitting: \\n(i) Increase model complexity\\n(ii) Increase the number of features, performing feature engineering\\n(iii) Remove noise from the data.\\n(iv) Increase the number of epochs or increase the duration of training to get better results.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
        "'''Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. \n",
        "When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. \n",
        "Then the model does not categorize the data correctly, because of too many details and noise.\n",
        "A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
        "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. \n",
        "Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. \n",
        "The overfitted model has low bias and high variance.The chances of occurrence of overfitting increase as much we provide training to our model. \n",
        "It means the more we train our model, the more chances of occurring the overfitted model.\n",
        "Overfitting is the main problem that occurs in supervised learning.\n",
        "Reasons for Overfitting are as follows:\n",
        "(i) High variance and low bias \n",
        "(ii) The model is too complex\n",
        "(iii) The size of the training data \n",
        "Techniques to reduce overfitting:\n",
        "(i) Increase training data.\n",
        "(ii) Reduce model complexity.\n",
        "(iii) Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "(iv) Ridge Regularization and Lasso Regularization\n",
        "(v) Use dropout for neural networks to tackle overfitting.\n",
        "\n",
        "Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, \n",
        "i.e., it only performs well on training data but performs poorly on testing data. \n",
        "Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. \n",
        "Underfitting can be avoided by using more data and also reducing the features by feature selection. \n",
        "Reasons for Underfitting:\n",
        "(i) High bias and low variance \n",
        "(ii) The size of the training dataset used is not enough.\n",
        "(iii) The model is too simple.\n",
        "(iv) Training data is not cleaned and also contains noise in it.\n",
        "Techniques to reduce underfitting: \n",
        "(i) Increase model complexity\n",
        "(ii) Increase the number of features, performing feature engineering\n",
        "(iii) Remove noise from the data.\n",
        "(iv) Increase the number of epochs or increase the duration of training to get better results.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2: How can we reduce overfitting? Explain in brief.\n",
        "'''The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and \n",
        "therefore they can really build unrealistic models. \n",
        "A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees.\n",
        "Techniques to reduce overfitting:\n",
        "(i) Increase training data.\n",
        "(ii) Reduce model complexity.\n",
        "(iii) Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "(iv) Ridge Regularization and Lasso Regularization\n",
        "(v) Use dropout for neural networks to tackle overfitting. '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2-qrsp8hC3YC",
        "outputId": "3458da71-e543-4b93-847f-87695434fa33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and \\ntherefore they can really build unrealistic models. \\nA solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees.\\nTechniques to reduce overfitting:\\n(i) Increase training data.\\n(ii) Reduce model complexity.\\n(iii) Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\\n(iv) Ridge Regularization and Lasso Regularization\\n(v) Use dropout for neural networks to tackle overfitting. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "'''Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. \n",
        "To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. \n",
        "As a result, it may fail to find the best fit of the dominant trend in the data.\n",
        "In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
        "An underfitted model has high bias and low variance.\n",
        "How to avoid underfitting:\n",
        "(i) By increasing the training time of the model.\n",
        "(ii) By increasing the number of features.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "lpdinxIRDdlI",
        "outputId": "ec0d8d9b-59d2-4087-ab4c-f4b290ef791f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. \\nTo avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. \\nAs a result, it may fail to find the best fit of the dominant trend in the data.\\nIn the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\\nAn underfitted model has high bias and low variance.\\nHow to avoid underfitting:\\n(i) By increasing the training time of the model.\\n(ii) By increasing the number of features.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
        "'''(i) Bias: Assumptions made by a model to make a function easier to learn. It is actually the error rate of the training data. \n",
        "When the error rate has a high value, we call it High Bias and when the error rate has a low value, we call it low Bias.\n",
        "(ii) Variance:  The difference between the error rate of training data and testing data is called variance. \n",
        "If the difference is high then it’s called high variance and when the difference of errors is low then it’s called low variance. Usually, we want to make a low variance for generalized our model.\n",
        "\n",
        "Bias Variance Tradeoff:\n",
        "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. \n",
        "If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. \n",
        "In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
        "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
        "The best fit will be given by hypothesis on the tradeoff point\n",
        "\n",
        "The goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.\n",
        "\n",
        "You can see a general trend in the examples above:\n",
        "\n",
        "Linear machine learning algorithms often have a high bias but a low variance.\n",
        "Nonlinear machine learning algorithms often have a low bias but a high variance.\n",
        "The parameterization of machine learning algorithms is often a battle to balance out bias and variance.\n",
        "\n",
        "There is no escaping the relationship between bias and variance in machine learning.\n",
        "Increasing the bias will decrease the variance.\n",
        "Increasing the variance will decrease the bias.\n",
        "There is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "UO4Nl4jaEqh9",
        "outputId": "eb9a4cd8-8317-4961-bd49-db1e6eec36b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(i) Bias: Assumptions made by a model to make a function easier to learn. It is actually the error rate of the training data. \\nWhen the error rate has a high value, we call it High Bias and when the error rate has a low value, we call it low Bias.\\n(ii) Variance:  The difference between the error rate of training data and testing data is called variance. \\nIf the difference is high then it’s called high variance and when the difference of errors is low then it’s called low variance. Usually, we want to make a low variance for generalized our model.\\n\\nBias Variance Tradeoff:\\nIf the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. \\nIf algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. \\nIn the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\\nThis tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\\nThe best fit will be given by hypothesis on the tradeoff point\\n\\nThe goal of any supervised machine learning algorithm is to achieve low bias and low variance. In turn the algorithm should achieve good prediction performance.\\n\\nYou can see a general trend in the examples above:\\n\\nLinear machine learning algorithms often have a high bias but a low variance.\\nNonlinear machine learning algorithms often have a low bias but a high variance.\\nThe parameterization of machine learning algorithms is often a battle to balance out bias and variance.\\n\\nThere is no escaping the relationship between bias and variance in machine learning.\\nIncreasing the bias will decrease the variance.\\nIncreasing the variance will decrease the bias.\\nThere is a trade-off at play between these two concerns and the algorithms you choose and the way you choose to configure them are finding different balances in this trade-off for your problem'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?\n",
        "'''Learning curves plot the training and validation loss of a sample of training examples by incrementally adding new training examples. \n",
        "Learning curves help us in identifying whether adding additional training examples would improve the validation score (score on unseen data). \n",
        "If a model is overfit, then adding additional training examples might improve the model performance on unseen data. \n",
        "Similarly, if a model is underfit, then adding training examples doesn’t help. ‘learning_curve’ method can be imported from Scikit-Learn’s ‘model_selection’ module.\n",
        "We’ll use the ‘learn_curve’ function to get a good fit model by setting the inverse regularization variable/parameter ‘c’ to 1.\n",
        "If a model has low bias and high varaince, it is said to be over fitting.\n",
        "If a model has high bias and low variance, it is said to be under fitting.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "NMu59AwhHKix",
        "outputId": "88e0a671-5369-4eb9-9905-fe29323e0d89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Learning curves plot the training and validation loss of a sample of training examples by incrementally adding new training examples. \\nLearning curves help us in identifying whether adding additional training examples would improve the validation score (score on unseen data). \\nIf a model is overfit, then adding additional training examples might improve the model performance on unseen data. \\nSimilarly, if a model is underfit, then adding training examples doesn’t help. ‘learning_curve’ method can be imported from Scikit-Learn’s ‘model_selection’ module.\\nWe’ll use the ‘learn_curve’ function to get a good fit model by setting the inverse regularization variable/parameter ‘c’ to 1.\\nIf a model has low bias and high varaince, it is said to be over fitting.\\nIf a model has high bias and low variance, it is said to be under fitting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
        "'''(i) Bias: Assumptions made by a model to make a function easier to learn. It is actually the error rate of the training data. \n",
        "When the error rate has a high value, we call it High Bias and when the error rate has a low value, we call it low Bias.\n",
        "(ii) Variance:  The difference between the error rate of training data and testing data is called variance. \n",
        "If the difference is high then it’s called high variance and when the difference of errors is low then it’s called low variance. Usually, we want to make a low variance for generalized our model.\n",
        "If a model has low bias and high varaince, it is said to be over fitting.\n",
        "If a model has high bias and low variance, it is said to be under fitting.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "lQuAFYYyJ-c2",
        "outputId": "b4abf54f-b142-45a6-9cae-97af17f13784"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(i) Bias: Assumptions made by a model to make a function easier to learn. It is actually the error rate of the training data. \\nWhen the error rate has a high value, we call it High Bias and when the error rate has a low value, we call it low Bias.\\n(ii) Variance:  The difference between the error rate of training data and testing data is called variance. \\nIf the difference is high then it’s called high variance and when the difference of errors is low then it’s called low variance. Usually, we want to make a low variance for generalized our model.\\nIf a model has low bias and high varaince, it is said to be over fitting.\\nIf a model has high bias and low variance, it is said to be under fitting.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
        "'''Regularization is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting. \n",
        "This technique can be used in such a way that it will allow to maintain all variables or features in the model by reducing the magnitude of the variables. \n",
        "Hence, it maintains accuracy as well as a generalization of the model.\n",
        "The commonly used regularization techniques are : \n",
        "(i) L1 regularization-A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.\n",
        "(ii) L2 regularization-A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.\n",
        "(iii) Dropout regularization\n",
        "Ridge Regression:\n",
        "Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
        "Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n",
        "In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty.\n",
        "We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
        "Lasso Regression:\n",
        "Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
        "It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
        "Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
        "It is also called as L1 regularization. \n",
        "Key Difference between Ridge Regression and Lasso Regression\n",
        "Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\n",
        "Lasso regression helps to reduce the overfitting in the model as well as feature selection.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "JGjDKwExKEOW",
        "outputId": "6c9ba0fa-99ae-4a31-d600-9758c3eabe89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regularization is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting. \\nThis technique can be used in such a way that it will allow to maintain all variables or features in the model by reducing the magnitude of the variables. \\nHence, it maintains accuracy as well as a generalization of the model.\\nThe commonly used regularization techniques are : \\n(i) L1 regularization-A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.\\n(ii) L2 regularization-A regression model which uses L1 Regularization technique is called LASSO(Least Absolute Shrinkage and Selection Operator) regression.\\n(iii) Dropout regularization\\nRidge Regression:\\nRidge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\\nRidge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\\nIn this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty.\\nWe can calculate it by multiplying with the lambda to the squared weight of each individual feature.\\nLasso Regression:\\nLasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\\nIt is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\\nSince it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\\nIt is also called as L1 regularization. \\nKey Difference between Ridge Regression and Lasso Regression\\nRidge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\\nLasso regression helps to reduce the overfitting in the model as well as feature selection.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}