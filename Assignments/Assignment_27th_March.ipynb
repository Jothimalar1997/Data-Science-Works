{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "JIheHbNK9hKp",
        "outputId": "2591adb6-c3df-4adb-9f51-4ee159c73ed2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'R-squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. \\nIn other words, r-squared shows how well the data fit the regression model (the goodness of fit).\\nR-squared is always between 0 and 100%: 0% represents a model that does not explain any of the variations in the response variable around its mean, while 100% represents a model that explains all of it.\\nR-squared is calculated as follows:\\n(i) Calculate the mean of the dependent variable (Y).\\n(ii) Calculate the predicted value of Y using the regression equation for each value of X.\\n(iii) Calculate the difference between each predicted value and Y (called residuals).\\n(iv) Square each residual and add them up.\\n(v) Divide this sum by the total sum of squares (TSS), which is calculated by subtracting the mean of Y from each observed Y value, squaring them, and adding them up.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
        "'''R-squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. \n",
        "In other words, r-squared shows how well the data fit the regression model (the goodness of fit).\n",
        "R-squared is always between 0 and 100%: 0% represents a model that does not explain any of the variations in the response variable around its mean, while 100% represents a model that explains all of it.\n",
        "R-squared is calculated as follows:\n",
        "(i) Calculate the mean of the dependent variable (Y).\n",
        "(ii) Calculate the predicted value of Y using the regression equation for each value of X.\n",
        "(iii) Calculate the difference between each predicted value and Y (called residuals).\n",
        "(iv) Square each residual and add them up.\n",
        "(v) Divide this sum by the total sum of squares (TSS), which is calculated by subtracting the mean of Y from each observed Y value, squaring them, and adding them up.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
        "'''Adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. \n",
        "In other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
        "The formula for adjusted R-squared is: Adjusted R2 = 1 – [ (1-R2)* (n-1)/ (n-k-1)] where: R2: The R2 of the model. n: The number of observations. k: The number of predictor variables.\n",
        "The most obvious difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the stock index and R-squared does not. \n",
        "R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable or variables in a regression model.\n",
        "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9j8JO1hf-Tpc",
        "outputId": "84e5420a-7c2b-41b4-f105-5dd655cea44b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. \\nIn other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\\nThe formula for adjusted R-squared is: Adjusted R2 = 1 – [ (1-R2)* (n-1)/ (n-k-1)] where: R2: The R2 of the model. n: The number of observations. k: The number of predictor variables.\\nThe most obvious difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the stock index and R-squared does not. \\nR-squared is a statistical measure that represents the proportion of the variance for a dependent variable that’s explained by an independent variable or variables in a regression model.\\nAdjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. When is it more appropriate to use adjusted R-squared?\n",
        "'''Adjusted R-squared is used to determine how reliable the correlation is and how much is determined by the addition of independent variables. \n",
        "In a portfolio model that has more independent variables, adjusted R-squared will help determine how much of the correlation with the index is due to the addition of those variables.\n",
        "Adjusted R-squared tells us how well a set of predictor variables is able to explain the variation in the response variable, adjusted for the number of predictors in a model. \n",
        "Because of the way it’s calculated, adjusted R-squared can be used to compare the fit of regression models with different numbers of predictor variables.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "db9tYAcJ-fJE",
        "outputId": "bd0a9b39-c371-45f7-9732-0a7b17a0e969"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Adjusted R-squared is used to determine how reliable the correlation is and how much is determined by the addition of independent variables. \\nIn a portfolio model that has more independent variables, adjusted R-squared will help determine how much of the correlation with the index is due to the addition of those variables.\\nAdjusted R-squared tells us how well a set of predictor variables is able to explain the variation in the response variable, adjusted for the number of predictors in a model. \\nBecause of the way it’s calculated, adjusted R-squared can be used to compare the fit of regression models with different numbers of predictor variables.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
        "'''In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are used to evaluate the performance of regression models.\n",
        "(i) MAE represents the average difference between predicted and actual values. It is calculated by taking the average of absolute differences between predicted and actual values.\n",
        "(ii) MSE represents the average squared difference between predicted and actual values. It is calculated by taking the average of squared differences between predicted and actual values.\n",
        "(iii) RMSE is the square root of MSE. It is used to have the error in the same unit as the outcome variable for easier interpretation purposes.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8BJC-Geh_pwt",
        "outputId": "d241043f-e0f8-41e6-e4b6-69818f52c966"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are used to evaluate the performance of regression models.\\n(i) MAE represents the average difference between predicted and actual values. It is calculated by taking the average of absolute differences between predicted and actual values.\\n(ii) MSE represents the average squared difference between predicted and actual values. It is calculated by taking the average of squared differences between predicted and actual values.\\n(iii) RMSE is the square root of MSE. It is used to have the error in the same unit as the outcome variable for easier interpretation purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
        "'''(i) Mean Absolute Error (MAE) is easier to interpret than MSE and RMSE as it gives equal weight to all errors. However, it does not penalize large errors as much as MSE and RMSE do.\n",
        "(ii) Mean Squared Error (MSE) is differentiable, so it can be used as a loss function. However, it has a squared unit of output which makes it difficult to interpret.\n",
        "(iii) Root Mean Squared Error (RMSE) has the same units as the dependent variable (Y-axis) which makes it easier to interpret than MSE. However, it penalizes large errors more than MAE does.\n",
        "In summary, MAE is easier to interpret but does not penalize large errors as much as MSE and RMSE do. MSE is differentiable and can be used as a loss function but has a squared unit of output which makes it difficult \n",
        "to interpret. RMSE has the same units as the dependent variable (Y-axis) which makes it easier to interpret than MSE but penalizes large errors more than MAE does.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gqCejKw1Bljy",
        "outputId": "0dd9d7c1-5ecb-4843-915e-cf83ab0afd12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(i) Mean Absolute Error (MAE) is easier to interpret than MSE and RMSE as it gives equal weight to all errors. However, it does not penalize large errors as much as MSE and RMSE do.\\n(ii) Mean Squared Error (MSE) is differentiable, so it can be used as a loss function. However, it has a squared unit of output which makes it difficult to interpret.\\n(iii) Root Mean Squared Error (RMSE) has the same units as the dependent variable (Y-axis) which makes it easier to interpret than MSE. However, it penalizes large errors more than MAE does.\\nIn summary, MAE is easier to interpret but does not penalize large errors as much as MSE and RMSE do. MSE is differentiable and can be used as a loss function but has a squared unit of output which makes it difficult \\nto interpret. RMSE has the same units as the dependent variable (Y-axis) which makes it easier to interpret than MSE but penalizes large errors more than MAE does.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
        "'''Lasso and Ridge are both regularization techniques used in linear regression to prevent overfitting. \n",
        "The main difference between Lasso and Ridge regularization is that Lasso uses L1 regularization while Ridge uses L2 regularization.\n",
        "L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. \n",
        "This means that Lasso can shrink some coefficients to zero, effectively performing feature selection. \n",
        "On the other hand, Ridge regression will not eliminate any features but will instead shrink all coefficients towards zero.\n",
        "In general, if you have a large number of features and you believe that only a small subset of them are important, then Lasso might be more appropriate. \n",
        "If you have a small number of features or you believe that all features are important, then Ridge might be more appropriate.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3U-2paqzCZ9X",
        "outputId": "dcbc703b-2393-4a79-aca8-3266c06451e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lasso and Ridge are both regularization techniques used in linear regression to prevent overfitting. \\nThe main difference between Lasso and Ridge regularization is that Lasso uses L1 regularization while Ridge uses L2 regularization.\\nL1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients while L2 regularization adds a penalty equal to the square of the magnitude of coefficients. \\nThis means that Lasso can shrink some coefficients to zero, effectively performing feature selection. \\nOn the other hand, Ridge regression will not eliminate any features but will instead shrink all coefficients towards zero.\\nIn general, if you have a large number of features and you believe that only a small subset of them are important, then Lasso might be more appropriate. \\nIf you have a small number of features or you believe that all features are important, then Ridge might be more appropriate.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
        "'''Regularized linear models are a type of linear regression model that adds a penalty term to the loss function to prevent overfitting. \n",
        "Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor generalization performance on new data. \n",
        "Regularization helps to reduce overfitting by adding a penalty term to the loss function that discourages large weights for the model parameters.\n",
        "For example, consider a linear regression model that predicts housing prices based on features such as square footage, number of bedrooms, and location. \n",
        "If we use a regularized linear model with L2 regularization, the loss function would be modified to include a penalty term proportional to the sum of squares of the model parameters. \n",
        "This encourages the model to use smaller weights for each feature, which can help prevent overfitting.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "dmgM2bOuDF54",
        "outputId": "629f43b7-b267-4381-c93e-9b92e80ded16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regularized linear models are a type of linear regression model that adds a penalty term to the loss function to prevent overfitting. \\nOverfitting occurs when a model is too complex and fits the training data too closely, resulting in poor generalization performance on new data. \\nRegularization helps to reduce overfitting by adding a penalty term to the loss function that discourages large weights for the model parameters.\\nFor example, consider a linear regression model that predicts housing prices based on features such as square footage, number of bedrooms, and location. \\nIf we use a regularized linear model with L2 regularization, the loss function would be modified to include a penalty term proportional to the sum of squares of the model parameters. \\nThis encourages the model to use smaller weights for each feature, which can help prevent overfitting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
        "'''Regularized linear models are a type of regression model that adds a penalty term to the loss function to prevent overfitting. \n",
        "However, they may not always be the best choice for regression analysis. Here are some limitations of regularized linear models:\n",
        "(i) Regularization can be too restrictive and lead to underfitting. This can happen when the regularization parameter is set too high or when there are too few predictors in the model.\n",
        "(ii) Regularization can also be too lenient and lead to overfitting. This can happen when the regularization parameter is set too low or when there are too many predictors in the model.\n",
        "(iii) Regularization assumes that all predictors have equal importance. This may not be true in practice, and some predictors may be more important than others.\n",
        "(iv) Regularization can be computationally expensive, especially when there are many predictors in the model.\n",
        "(v) Regularization may not work well with highly correlated predictors.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ocaq3VxMDyK_",
        "outputId": "96ad13ed-6bed-4c3c-924b-e503bdbfe571"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regularized linear models are a type of regression model that adds a penalty term to the loss function to prevent overfitting. \\nHowever, they may not always be the best choice for regression analysis. Here are some limitations of regularized linear models:\\n(i) Regularization can be too restrictive and lead to underfitting. This can happen when the regularization parameter is set too high or when there are too few predictors in the model.\\n(ii) Regularization can also be too lenient and lead to overfitting. This can happen when the regularization parameter is set too low or when there are too many predictors in the model.\\n(iii) Regularization assumes that all predictors have equal importance. This may not be true in practice, and some predictors may be more important than others.\\n(iv) Regularization can be computationally expensive, especially when there are many predictors in the model.\\n(v) Regularization may not work well with highly correlated predictors.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?'''\n",
        "'''In your case, Model A has an RMSE of 10, while Model B has an MAE of 8. \n",
        "Since both models have different evaluation metrics, it’s not straightforward to compare them. \n",
        "However, if we assume that both models have similar error distributions, then we can say that Model B has a better performance because it has a lower error value than Model A.\n",
        "Limitations: \n",
        "They don’t tell us anything about the direction of errors (i.e., overestimation or underestimation).\n",
        "Also, they don’t tell us anything about the distribution of errors (i.e., whether they are normally distributed or not).\n",
        " Therefore, it’s important to use multiple evaluation metrics to get a better understanding of model performance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "DSOTQs-ZEqDY",
        "outputId": "62bd4587-2bf5-4cba-dde6-08f1273c17f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In your case, Model A has an RMSE of 10, while Model B has an MAE of 8. \\nSince both models have different evaluation metrics, it’s not straightforward to compare them. \\nHowever, if we assume that both models have similar error distributions, then we can say that Model B has a better performance because it has a lower error value than Model A.\\nLimitations: \\nThey don’t tell us anything about the direction of errors (i.e., overestimation or underestimation).\\nAlso, they don’t tell us anything about the distribution of errors (i.e., whether they are normally distributed or not).\\n Therefore, it’s important to use multiple evaluation metrics to get a better understanding of model performance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as thebetter performer, and why? Are there any trade-offs or limitations to your choice of regularization method?'''\n",
        "'''In general, Ridge regression is better suited when there are many variables in the model and all of them are important predictors. \n",
        "Lasso regression is better suited when there are many variables in the model but only a few of them are important predictors. \n",
        "Based on the regularization parameter values given in the problem, we can say that Model A (Ridge regression) has a lower regularization parameter value than Model B (Lasso regression). \n",
        "This means that Model A will have less shrinkage than Model B and will be less likely to underfit. However, Model A may still overfit if there are too many variables in the model. \n",
        "Model B will have more shrinkage than Model A and will be less likely to overfit. \n",
        "However, if there are many important predictors in the model that have small coefficients, then Lasso may exclude them from the model altogether.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Tr9FMSyYFB73",
        "outputId": "c01904fb-ba4f-46f0-f61d-4b9ecea7967a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In general, Ridge regression is better suited when there are many variables in the model and all of them are important predictors. \\nLasso regression is better suited when there are many variables in the model but only a few of them are important predictors. \\nBased on the regularization parameter values given in the problem, we can say that Model A (Ridge regression) has a lower regularization parameter value than Model B (Lasso regression). \\nThis means that Model A will have less shrinkage than Model B and will be less likely to underfit. However, Model A may still overfit if there are too many variables in the model. \\nModel B will have more shrinkage than Model A and will be less likely to overfit. \\nHowever, if there are many important predictors in the model that have small coefficients, then Lasso may exclude them from the model altogether.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}