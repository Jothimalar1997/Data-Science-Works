{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "NFRHROh-1yIc",
        "outputId": "ad750b6e-7004-4429-d41b-3b26ad553096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ridge regression belongs a class of regression tools that use L2 regularization.\\nL2 regularization adds an L2 penalty, which equals the square of the magnitude of coefficients. \\nAll coefficients are shrunk by the same factor (so none are eliminated). Unlike L1 regularization, L2 will not result in sparse models.\\nRidge Regression is an adaptation of the popular and widely used linear regression algorithm. \\nIt enhances regular linear regression by slightly changing its cost function, which results in less overfit models. \\nRidge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), \\nbut by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
        "'''Ridge regression belongs a class of regression tools that use L2 regularization.\n",
        "L2 regularization adds an L2 penalty, which equals the square of the magnitude of coefficients. \n",
        "All coefficients are shrunk by the same factor (so none are eliminated). Unlike L1 regularization, L2 will not result in sparse models.\n",
        "Ridge Regression is an adaptation of the popular and widely used linear regression algorithm. \n",
        "It enhances regular linear regression by slightly changing its cost function, which results in less overfit models. \n",
        "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), \n",
        "but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator. '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. What are the assumptions of Ridge Regression?\n",
        "'''The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. \n",
        "However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9zLrFmmX24-8",
        "outputId": "318db785-0a90-4c1a-d425-63ec6e784fc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. \\nHowever, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
        "'''The value of the tuning parameter (lambda) in ridge regression can be selected using various techniques. \n",
        "One such technique is to use cross-validation. \n",
        "In this technique, we randomly divide the data into k equal parts (or “folds”). For each value of lambda, we fit a model using k-1 of the folds (called the “training set”) and \n",
        "compute the prediction error on the remaining fold (called the “validation set”). We then average the prediction errors for each value of lambda and choose the value that gives the smallest average error.\n",
        "Another technique is to use a grid search2. In this technique, we fit a model for each value of lambda in a grid of possible values and choose the value that gives the best performance on a validation set.\n",
        "There are other techniques as well such as generalized cross-validation (GCV) and unbiased risk estimation (URE).'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ZaWZ5laF3Q5L",
        "outputId": "43fbcb68-a055-4496-a43e-5cccabf1c011"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The value of the tuning parameter (lambda) in ridge regression can be selected using various techniques. \\nOne such technique is to use cross-validation. \\nIn this technique, we randomly divide the data into k equal parts (or “folds”). For each value of lambda, we fit a model using k-1 of the folds (called the “training set”) and \\ncompute the prediction error on the remaining fold (called the “validation set”). We then average the prediction errors for each value of lambda and choose the value that gives the smallest average error.\\nAnother technique is to use a grid search2. In this technique, we fit a model for each value of lambda in a grid of possible values and choose the value that gives the best performance on a validation set.\\nThere are other techniques as well such as generalized cross-validation (GCV) and unbiased risk estimation (URE).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
        "'''Yes, ridge regression can be used for feature selection by shrinking the coefficients of irrelevant or redundant features towards zero.\n",
        "However, Lasso regression is typically a better choice for feature selection because it can set coefficients to exactly zero, effectively removing irrelevant features from the model.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Z8JaKoaM34wq",
        "outputId": "7cb5e517-aa76-4aa2-ccb4-f571989d3945"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, ridge regression can be used for feature selection by shrinking the coefficients of irrelevant or redundant features towards zero.\\nHowever, Lasso regression is typically a better choice for feature selection because it can set coefficients to exactly zero, effectively removing irrelevant features from the model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
        "'''Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization.\n",
        " When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n",
        " The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
        "It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
        "It reduces the model complexity by coefficient shrinkage'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2OUR5qJw4PCT",
        "outputId": "62a37632-a1b5-4609-bd59-a6943e3c99b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization.\\n When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\\n The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\\nIt shrinks the parameters. Therefore, it is used to prevent multicollinearity\\nIt reduces the model complexity by coefficient shrinkage'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
        "'''Yes, Ridge Regression can handle both categorical and continuous independent variables. \n",
        "However, when using Ridge regression with categorical predictors, it might make sense to standardize all variables, including the binary ones1.\n",
        "In linear regression, the independent variables can be categorical and/or continuous.\n",
        " But when you fit the model, if you have more than two categories in the categorical independent variable, make sure you are creating dummy variables2.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3tmaT-_j45Lr",
        "outputId": "08ad1172-9139-4256-acd1-7e873e76bf5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, Ridge Regression can handle both categorical and continuous independent variables. \\nHowever, when using Ridge regression with categorical predictors, it might make sense to standardize all variables, including the binary ones1.\\nIn linear regression, the independent variables can be categorical and/or continuous.\\n But when you fit the model, if you have more than two categories in the categorical independent variable, make sure you are creating dummy variables2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. How do you interpret the coefficients of Ridge Regression?\n",
        "'''Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. \n",
        "The Ridge regression will penalize your coefficients, such that those that are the least effective in your estimation will “shrink” the fastest.\n",
        "The Ridge regression plot can be used to interpret the coefficients of Ridge Regression. \n",
        "The plot shows how the magnitude of each coefficient changes as we increase the value of alpha. \n",
        "The alpha parameter controls the amount of shrinkage: the higher the value of alpha, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "GsxIcRY25bss",
        "outputId": "6e33a0de-1013-4b3e-951e-151f4a5e86e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. \\nThe Ridge regression will penalize your coefficients, such that those that are the least effective in your estimation will “shrink” the fastest.\\nThe Ridge regression plot can be used to interpret the coefficients of Ridge Regression. \\nThe plot shows how the magnitude of each coefficient changes as we increase the value of alpha. \\nThe alpha parameter controls the amount of shrinkage: the higher the value of alpha, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
        "'''Yes, Ridge Regression can be used for time-series data analysis. \n",
        "If you introduce an L2 penalty to a standard linear regression model then you will have a ridge regression model that can be used with a continuous outcome. \n",
        "The continuous outcome ridge regression model is perhaps the most common type of ridge regression model around.\n",
        "In time-series modeling, Ridge Regression is used for forecasting and finding the causal effect relationship between variables.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0qGBdB9W6Ef0",
        "outputId": "f54fcd65-809e-4c1c-c8bf-fcbc23f242fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, Ridge Regression can be used for time-series data analysis. \\nIf you introduce an L2 penalty to a standard linear regression model then you will have a ridge regression model that can be used with a continuous outcome. \\nThe continuous outcome ridge regression model is perhaps the most common type of ridge regression model around.\\nIn time-series modeling, Ridge Regression is used for forecasting and finding the causal effect relationship between variables.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}