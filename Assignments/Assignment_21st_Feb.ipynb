{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "loxTo81dsrJ4",
        "outputId": "0c801364-c93c-45fb-ab4f-76ca15f1c979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that \\nit can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from \\nscratch. \\n         \\nWeb scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. \\nThe scraper, on the other hand, is a specific tool created to extract data from the website. \\n\\nThe design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.\\nMany large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to \\naccess large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\\n\\nUses of web scrapping- Data mining,Content aggregation,News Monitoring, Price comparison.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1. Web Scraping:\n",
        "'''Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that \n",
        "it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular API’s or even creating your code for web scraping from \n",
        "scratch. \n",
        "         \n",
        "Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. \n",
        "The scraper, on the other hand, is a specific tool created to extract data from the website. \n",
        "\n",
        "The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.\n",
        "Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to \n",
        "access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
        "\n",
        "Uses of web scrapping- Data mining,Content aggregation,News Monitoring, Price comparison.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. What are the different methods used for Web Scraping?\n",
        "'''The most common methods used for Web Scraping are\n",
        "(i) Human copy-and-paste.\n",
        "(ii) Text pattern matching.\n",
        "(iii) HTTP programming.\n",
        "(iv) HTML parsing.\n",
        "(v) DOM parsing.\n",
        "(vi) Vertical aggregation.\n",
        "(vii) Semantic annotation recognizing.\n",
        "(viii) Computer vision web-page analysis.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5FP2A-Q2xC1H",
        "outputId": "423dca8a-ba4f-442c-aed1-16b9283c9120"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The most common methods used for Web Scraping are\\n(i) Human copy-and-paste.\\n(ii) Text pattern matching.\\n(iii) HTTP programming.\\n(iv) HTML parsing.\\n(v) DOM parsing.\\n(vi) Vertical aggregation.\\n(vii) Semantic annotation recognizing.\\n(viii) Computer vision web-page analysis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. What is Beautiful Soup? Why is it used?\n",
        "'''Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n",
        "Some key features that make beautiful soup unique are:\n",
        "(i) Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
        "(ii) It automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
        "(iii) It sits on top of popular Python parsers like lxml and html5lib, which allows​ us to try out different parsing strategies or trade speed for flexibility.\n",
        "Uses- This library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "u28VifT2RG0k",
        "outputId": "411854b6-43c9-4eb8-a6a0-5d3211b1b124"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\\nSome key features that make beautiful soup unique are:\\n(i) Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\\n(ii) It automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\\n(iii) It sits on top of popular Python parsers like lxml and html5lib, which allows\\u200b us to try out different parsing strategies or trade speed for flexibility.\\nUses- This library helps with isolating titles and links from webpages. It can extract all of the text from \\u200bHTML tags, and alter the HTML \\u200bin the document with which we’re working.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Why is flask used in this Web Scraping project?\n",
        "'''Flask is a web framework written in python used for easy and fast web application development, and for configuring backend applications with the frontend in an easy way. \n",
        "It gives complete control to developers on how to access data. \n",
        "Flask is based on Werkzeug’s(WSGI) toolkit and Jinja templating engine.\n",
        "Flask provides different libraries, tools, and modules, and many functionalities like handling user requests, routing, sessions, form validation, etc that can be easily used to develop a blog website or any commercial website, etc.\n",
        "There is no requirement of any boilerplate code in a flask that preserves your application’s main function.\n",
        "A major advantage of using flask is easy setup, and freedom to build a structure of web application as per your rules. It means flask is not bound as Django to use a specific set of rules.'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "T27eG9_1R-sD",
        "outputId": "d61e6771-2c25-4574-d5f6-de97f2c764d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Flask is a web framework written in python used for easy and fast web application development, and for configuring backend applications with the frontend in an easy way. \\nIt gives complete control to developers on how to access data. \\nFlask is based on Werkzeug’s(WSGI) toolkit and Jinja templating engine.\\nFlask provides different libraries, tools, and modules, and many functionalities like handling user requests, routing, sessions, form validation, etc that can be easily used to develop a blog website or any commercial website, etc.\\nThere is no requirement of any boilerplate code in a flask that preserves your application’s main function.\\nA major advantage of using flask is easy setup, and freedom to build a structure of web application as per your rules. It means flask is not bound as Django to use a specific set of rules.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service:\n",
        "'''Code pipeline and Beanstalk are the two AWS servives used in this project.\n",
        "(i) AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n",
        "It is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. \n",
        "CodePipeline automates the steps required to release your software changes continuously. \n",
        "(ii) With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. \n",
        "Elastic Beanstalk reduces management complexity without restricting choice or control. \n",
        "You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\n",
        "Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. \n",
        "When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.\n",
        "We can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "7NfgZdugTG3N",
        "outputId": "c859f884-9414-4492-a16d-a3cc952a55e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Code pipeline and Beanstalk are the two AWS servives used in this project.\\n(i) AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\\nIt is a continuous delivery service you can use to model, visualize, and automate the steps required to release your software. You can quickly model and configure the different stages of a software release process. \\nCodePipeline automates the steps required to release your software changes continuously. \\n(ii) With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. \\nElastic Beanstalk reduces management complexity without restricting choice or control. \\nYou simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\\nElastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. \\nWhen you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.\\nWe can also perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}