{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "OFLEJu7mPI9v",
        "outputId": "3b7f2dfd-b93b-446d-9280-bcc60b1876b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The k-nearest neighbors algorithm (KNN) is a non-parametric, supervised learning classifier that uses proximity to make classifications or predictions about the grouping of an individual data point. \\nThe KNN algorithm assumes that similar things exist in close proximity and that similar things are near to each other. \\nThe input consists of the k closest training examples in a data set. \\nThe output depends on whether KNN is used for classification or regression. \\nIn KNN classification, the output is a class membership. \\nAn object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. \\nIn KNN regression, the output is the property value for the object.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1. What is the KNN algorithm?\n",
        "'''The k-nearest neighbors algorithm (KNN) is a non-parametric, supervised learning classifier that uses proximity to make classifications or predictions about the grouping of an individual data point. \n",
        "The KNN algorithm assumes that similar things exist in close proximity and that similar things are near to each other. \n",
        "The input consists of the k closest training examples in a data set. \n",
        "The output depends on whether KNN is used for classification or regression. \n",
        "In KNN classification, the output is a class membership. \n",
        "An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors. \n",
        "In KNN regression, the output is the property value for the object.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. How do you choose the value of K in KNN?\n",
        "'''The value of k in KNN is not easy to find1. A small value of k means that noise will have a higher influence on the result and a large value makes it computationally expensive. \n",
        "Data scientists usually choose an odd number if the number of classes is 2. \n",
        "Another simple approach to select k is set k=sqrt (n). \n",
        "To select the K for your data, you can run the KNN algorithm several times with different values of K and choose the K which reduces the number of errors while maintaining the algorithm’s ability to accurately make predictions. \n",
        "It is wise not to choose k=1 as it will lead to overfitting. The best thing to do is to treat k as a hyperparameter and find its value during the tuning phase.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bUYlQMN8PwMt",
        "outputId": "3cd79ca6-e965-42e1-82d8-d5d17b6e0f75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The value of k in KNN is not easy to find1. A small value of k means that noise will have a higher influence on the result and a large value makes it computationally expensive. \\nData scientists usually choose an odd number if the number of classes is 2. \\nAnother simple approach to select k is set k=sqrt (n). \\nTo select the K for your data, you can run the KNN algorithm several times with different values of K and choose the K which reduces the number of errors while maintaining the algorithm’s ability to accurately make predictions. \\nIt is wise not to choose k=1 as it will lead to overfitting. The best thing to do is to treat k as a hyperparameter and find its value during the tuning phase.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "'''KNN regression tries to predict the value of the output variable by using a local average while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability. \n",
        "You can think of KNeighborsRegressor as a method/function used for predicting a value (just as simple linear regression does but using a different way/algorithm). \n",
        "KNeighborsClassifier has the goal of putting data into different classifications using the KNN algorithm2. The KNeighborsClassifier essentially performs a majority vote. \n",
        "The prediction for the query x is 0, which means ‘happy’. So this is the way to go here. The KNeighborsRegressor instead computes the mean of the nearest neighbor labels.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Ce1sl7k5QL40",
        "outputId": "22abf791-2578-4bce-ad0a-f14de1067bd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KNN regression tries to predict the value of the output variable by using a local average while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability. \\nYou can think of KNeighborsRegressor as a method/function used for predicting a value (just as simple linear regression does but using a different way/algorithm). \\nKNeighborsClassifier has the goal of putting data into different classifications using the KNN algorithm2. The KNeighborsClassifier essentially performs a majority vote. \\nThe prediction for the query x is 0, which means ‘happy’. So this is the way to go here. The KNeighborsRegressor instead computes the mean of the nearest neighbor labels.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. How do you measure the performance of KNN?\n",
        "'''The performance of KNN can be measured using the following metrics:\n",
        "1) Accuracy: It is the most common metric used for classification problems. It measures the percentage of correctly classified instances.\n",
        "2) Precision: It measures the percentage of correctly classified instances among the predicted positive instances.\n",
        "3) Recall: It measures the percentage of correctly classified instances among the actual positive instances.\n",
        "4) F1-score: It is the harmonic mean of precision and recall.\n",
        "5) Confusion matrix: It is a table that summarizes the performance of a classification algorithm.\n",
        "6) ROC curve: It is a plot of true positive rate (TPR) against false positive rate (FPR) at different classification thresholds.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "5cpuagvfQfBU",
        "outputId": "aa8af463-3346-4672-afdf-47816e3a0d00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The performance of KNN can be measured using the following metrics:\\n1) Accuracy: It is the most common metric used for classification problems. It measures the percentage of correctly classified instances.\\n2) Precision: It measures the percentage of correctly classified instances among the predicted positive instances.\\n3) Recall: It measures the percentage of correctly classified instances among the actual positive instances.\\n4) F1-score: It is the harmonic mean of precision and recall.\\n5) Confusion matrix: It is a table that summarizes the performance of a classification algorithm.\\n6) ROC curve: It is a plot of true positive rate (TPR) against false positive rate (FPR) at different classification thresholds.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. What is the curse of dimensionality in KNN?\n",
        "'''The curse of dimensionality is a phenomenon that occurs when the number of dimensions increases and the available data becomes sparse. \n",
        "In KNN, the curse of dimensionality refers to the fact that as the number of dimensions increases, the distance between the nearest and farthest neighbors becomes almost the same. \n",
        "This makes it difficult to find meaningful nearest neighbors. \n",
        "As a result, KNN becomes less effective as the number of dimensions increases. To overcome this problem, you can use feature selection or dimensionality reduction techniques.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Hnm8hHFdQ1ek",
        "outputId": "58902edc-57bb-4e18-9389-38253795007e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The curse of dimensionality is a phenomenon that occurs when the number of dimensions increases and the available data becomes sparse. \\nIn KNN, the curse of dimensionality refers to the fact that as the number of dimensions increases, the distance between the nearest and farthest neighbors becomes almost the same. \\nThis makes it difficult to find meaningful nearest neighbors. \\nAs a result, KNN becomes less effective as the number of dimensions increases. To overcome this problem, you can use feature selection or dimensionality reduction techniques.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. How do you handle missing values in KNN?\n",
        "'''There are several ways to handle missing values in KNN:\n",
        "1) Deletion: You can delete the instances with missing values. However, this method can lead to a loss of information.\n",
        "2) Imputation: You can replace the missing values with the mean or median of the feature. This method can lead to biased results if the data is not missing at random.\n",
        "3) KNN imputation: You can use KNN to impute the missing values. In this method, you use KNN to find the k-nearest neighbors of the instance with missing values and then use their values to impute the missing values.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "83GCW4TxRE58",
        "outputId": "ab05fad7-ca37-4703-8e36-5c44cbb9ecec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are several ways to handle missing values in KNN:\\n1) Deletion: You can delete the instances with missing values. However, this method can lead to a loss of information.\\n2) Imputation: You can replace the missing values with the mean or median of the feature. This method can lead to biased results if the data is not missing at random.\\n3) KNN imputation: You can use KNN to impute the missing values. In this method, you use KNN to find the k-nearest neighbors of the instance with missing values and then use their values to impute the missing values.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "'''KNN regression tries to predict the value of the output variable by using a local average while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability. \n",
        "The choice between KNN classifier and regressor depends on the type of problem you are trying to solve. If you are trying to predict a continuous variable, then KNN regression is a better choice. \n",
        "On the other hand, if you are trying to classify instances into different classes, then KNN classification is a better choice.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "j_0HWS41Rd4g",
        "outputId": "f26d56a5-d1b6-41f2-fe1e-c5ca6ae06164"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'KNN regression tries to predict the value of the output variable by using a local average while KNN classification attempts to predict the class to which the output variable belongs by computing the local probability. \\nThe choice between KNN classifier and regressor depends on the type of problem you are trying to solve. If you are trying to predict a continuous variable, then KNN regression is a better choice. \\nOn the other hand, if you are trying to classify instances into different classes, then KNN classification is a better choice.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "'''The strengths and weaknesses of the KNN algorithm for classification and regression tasks are as follows:\n",
        "Strengths:\n",
        "1) KNN is simple and easy to understand.\n",
        "2) KNN can be used for both classification and regression tasks.\n",
        "3) KNN does not make any assumptions about the underlying data distribution.\n",
        "4) KNN can work well with small datasets.\n",
        "Weaknesses:\n",
        "1) KNN can be computationally expensive, especially when dealing with large datasets.\n",
        "2) KNN is sensitive to the choice of distance metric.\n",
        "3) KNN is sensitive to the value of k.\n",
        "4) KNN does not work well with high-dimensional data.\n",
        "To address these weaknesses, you can use the following techniques:\n",
        "1) Distance weighting: You can weight the distances based on their importance.\n",
        "2) Feature selection: You can select a subset of features that are most relevant to the problem.\n",
        "3) Dimensionality reduction: You can reduce the number of dimensions using techniques such as PCA or LDA.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "kmpIq84MRplS",
        "outputId": "a7dd3839-5c7c-4663-9ad1-5ec1a01f383d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The strengths and weaknesses of the KNN algorithm for classification and regression tasks are as follows:\\nStrengths:\\n1) KNN is simple and easy to understand.\\n2) KNN can be used for both classification and regression tasks.\\n3) KNN does not make any assumptions about the underlying data distribution.\\n4) KNN can work well with small datasets.\\nWeaknesses:\\n1) KNN can be computationally expensive, especially when dealing with large datasets.\\n2) KNN is sensitive to the choice of distance metric.\\n3) KNN is sensitive to the value of k.\\n4) KNN does not work well with high-dimensional data.\\nTo address these weaknesses, you can use the following techniques:\\n1) Distance weighting: You can weight the distances based on their importance.\\n2) Feature selection: You can select a subset of features that are most relevant to the problem.\\n3) Dimensionality reduction: You can reduce the number of dimensions using techniques such as PCA or LDA.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "'''The Euclidean distance is the straight-line distance between two points in Euclidean space. \n",
        "It is calculated as the square root of the sum of the squared differences between the coordinates of the two points. \n",
        "The Manhattan distance is the sum of the absolute differences between the coordinates of the two points. \n",
        "The main difference between Euclidean distance and Manhattan distance is that Euclidean distance is sensitive to the scale of the variables while Manhattan distance is not. \n",
        "In other words, if you have variables with different scales, then Euclidean distance may not work well. In such cases, you can use Manhattan distance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "SJFTg3mmSGmc",
        "outputId": "1240ce3c-9609-464b-b6a3-d4e14ce4f8c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Euclidean distance is the straight-line distance between two points in Euclidean space. \\nIt is calculated as the square root of the sum of the squared differences between the coordinates of the two points. \\nThe Manhattan distance is the sum of the absolute differences between the coordinates of the two points. \\nThe main difference between Euclidean distance and Manhattan distance is that Euclidean distance is sensitive to the scale of the variables while Manhattan distance is not. \\nIn other words, if you have variables with different scales, then Euclidean distance may not work well. In such cases, you can use Manhattan distance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q10. What is the role of feature scaling in KNN?\n",
        "'''Feature scaling is an important step in KNN because it helps to ensure that all the features are on the same scale. \n",
        "If the features are not on the same scale, then the distance metric used by KNN may be dominated by one or more features. \n",
        "This can lead to biased results. \n",
        "Feature scaling can be done using techniques such as min-max scaling or standardization. \n",
        "Min-max scaling scales the features to a fixed range (usually 0 to 1) while standardization scales the features to have zero mean and unit variance.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "EGxRbRZISVIk",
        "outputId": "62ad1802-8d9c-4fea-eed0-eaee0c9f629c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Feature scaling is an important step in KNN because it helps to ensure that all the features are on the same scale. \\nIf the features are not on the same scale, then the distance metric used by KNN may be dominated by one or more features. \\nThis can lead to biased results. \\nFeature scaling can be done using techniques such as min-max scaling or standardization. \\nMin-max scaling scales the features to a fixed range (usually 0 to 1) while standardization scales the features to have zero mean and unit variance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}