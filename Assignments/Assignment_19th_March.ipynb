{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNpS-0nq4s-F",
        "outputId": "a9b3a5dd-c108-4f36-a958-76a6f17d7cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.  ]\n",
            " [0.25 0.25]\n",
            " [0.5  0.5 ]\n",
            " [0.75 0.75]\n",
            " [1.   1.  ]]\n"
          ]
        }
      ],
      "source": [
        "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
        "'''Using Min-Max Scaling you can normalize the range of independent data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
        "It is another way of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. \n",
        "MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range.\n",
        "It scales the values to a specific value range without changing the shape of the original distribution.'''\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# create data\n",
        "data = [[11,2],[13,4],[15,6],[17,8],[19,10]]\n",
        "# scale features\n",
        "scaler=MinMaxScaler()\n",
        "model=scaler.fit(data)\n",
        "scaled_data=model.transform(data)\n",
        "# print scaled features\n",
        "print(scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
        "'''Unit Vector Scaling technique is done considering the whole feature vector to be of unit length,Unit vector scaling means dividing each component by the Euclidean length of the vector (L2 Norm),Unit Vector technique produces values of range [0,1].\n",
        "When dealing with features with hard boundaries, this is quite useful ex. when dealing with image data, the colors can range from only 0 to 255.\n",
        "The Normalizer rescales the vector for each sample to have unit norm, independently of the distribution of the samples.\n",
        "Min-Max scaling scales the value between 0 to 1 while Unit vector scales the value to 1.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XyXVyZZl52vw",
        "outputId": "7d4004b7-7fe3-4de6-f64f-f5c5a5aaeac4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Unit Vector Scaling technique is done considering the whole feature vector to be of unit length,Unit vector scaling means dividing each component by the Euclidean length of the vector (L2 Norm),Unit Vector technique produces values of range [0,1].\\nWhen dealing with features with hard boundaries, this is quite useful ex. when dealing with image data, the colors can range from only 0 to 255.\\nThe Normalizer rescales the vector for each sample to have unit norm, independently of the distribution of the samples.\\nMin-Max scaling scales the value between 0 to 1 while Unit vector scales the value to 1.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
        "'''Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a large dataset. \n",
        "It is a commonly used method in machine learning, data science, and other fields that deal with large datasets.\n",
        "It works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum.\n",
        "PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. \n",
        "Some real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels. \n",
        "It is a feature extraction technique, so it contains the important variables and drops the least important variable.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "eSueuHPCSSul",
        "outputId": "105745b2-8273-4116-9dba-e3efd07ac47a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a large dataset. \\nIt is a commonly used method in machine learning, data science, and other fields that deal with large datasets.\\nIt works on a condition that while the data in a higher dimensional space is mapped to data in a lower dimension space, the variance of the data in the lower dimensional space should be maximum.\\nPCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality. \\nSome real-world applications of PCA are image processing, movie recommendation system, optimizing the power allocation in various communication channels. \\nIt is a feature extraction technique, so it contains the important variables and drops the least important variable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
        "'''There are two options to reduce dimensionality:\n",
        "Feature elimination: we remove some features directly.\n",
        "Feature extraction: we keep the important fraction of all the features. We apply PCA to achieve this. Note that PCA is not the only method that does the feature extraction.\n",
        "PCA:\n",
        "PCA is a dimensionality reduction that identifies important relationships in our data, transforms the existing data based on these relationships, and then quantifies the importance of these relationships so we \n",
        "can keep the most important relationships and drop the others. \n",
        "To remember this definition, we can break it down into four steps:\n",
        "(i) We identify the relationship among features through a Covariance Matrix.\n",
        "(ii) Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\n",
        "(iii) Then we transform our data using Eigenvectors into principal components.\n",
        "(iv) Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "cOGFL_gUS4iE",
        "outputId": "322f090e-a873-49d4-a294-6f0b5c9be9f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are two options to reduce dimensionality:\\nFeature elimination: we remove some features directly.\\nFeature extraction: we keep the important fraction of all the features. We apply PCA to achieve this. Note that PCA is not the only method that does the feature extraction.\\nPCA:\\nPCA is a dimensionality reduction that identifies important relationships in our data, transforms the existing data based on these relationships, and then quantifies the importance of these relationships so we \\ncan keep the most important relationships and drop the others. \\nTo remember this definition, we can break it down into four steps:\\n(i) We identify the relationship among features through a Covariance Matrix.\\n(ii) Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\\n(iii) Then we transform our data using Eigenvectors into principal components.\\n(iv) Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data\n",
        "'''1) import MinMaxScaler class from sklearn.preprocessing.\n",
        "2) Create an instance for MinMaxScaler.\n",
        "3) With the help of fit and transform method, pass price, rating, and delivery time as arguments to the  MinMaxScaler instance and display the processed output.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gNPw_gi1T-Bi",
        "outputId": "f468492f-ade5-43bb-cc52-1b42ecd2d290"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1) import MinMaxScaler class from sklearn.preprocessing.\\n2) Create an instance for MinMaxScaler.\\n3) With the help of fit and transform method, pass price, rating, and delivery time as arguments to the  MinMaxScaler instance and display the processed output.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
        "'''(i) We identify the relationship among features through a Covariance Matrix.\n",
        "(ii) Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\n",
        "(iii) Then we transform our data using Eigenvectors into principal components.\n",
        "(iv) Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LzPERpg2VqAc",
        "outputId": "cee4bb5e-3c8c-496c-b352-fe1df004fcfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(i) We identify the relationship among features through a Covariance Matrix.\\n(ii) Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\\n(iii) Then we transform our data using Eigenvectors into principal components.\\n(iv) Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "scaler=MinMaxScaler()\n",
        "data=pd.DataFrame({'values':[1,5,10,15,20]})\n",
        "scaler.fit_transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPlI9KkIUncj",
        "outputId": "b2ca1874-6679-4662-91ff-763a9e7a78d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        ],\n",
              "       [0.21052632],\n",
              "       [0.47368421],\n",
              "       [0.73684211],\n",
              "       [1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "'''I will choose 2 principal components because height, weight, age are highly correlated and gender, blood pressure are highly correlated. So, 2 principle components will capture the variance explained by both the groups.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IPNBbb3cVNYa",
        "outputId": "bc5a4b2a-b3c4-4836-dd01-9ea7e5c6be59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I will choose 2 principal components because height, weight, age are highly correlated and gender, blood pressure are highly correlated. So, 2 principle components will capture the variance explained by both the groups.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}