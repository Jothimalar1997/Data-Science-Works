{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3UFa-Se_NjF"
      },
      "outputs": [],
      "source": [
        "## Q1. What is Gradient Boosting Regression?\n",
        "'''Gradient boosting regression is a machine learning technique used in regression and classification tasks among others. \n",
        "It produces a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. \n",
        "It explores the relationship between two or more variables and identifies important factors impacting the dependent variable. \n",
        "It builds the model in a stage-wise fashion and allows optimization of an arbitrary differentiable loss function. \n",
        "The number of decision trees used in the boosting stages is decided by the parameter n_estimators.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
        "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
        "performance using metrics such as mean squared error and R-squared.'''\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random data\n",
        "X = np.random.randn(100, 10)\n",
        "y = np.sum(X, axis=1) + np.random.randn(100)\n",
        "\n",
        "# Define our loss function and its gradient\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "\n",
        "def mse_grad(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / len(y_true)\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_trees=10, max_depth=3, learning_rate=0.1):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.trees = []\n",
        "        self.training_losses = []\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        # Consider the mean of the targets as our first guess\n",
        "        guess = np.mean(y)\n",
        "        \n",
        "        for i in range(self.n_trees):\n",
        "            # Compute the residuals of the previous guess\n",
        "            residuals = y - guess\n",
        "            \n",
        "            # Train a tree to predict the residuals\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.trees.append(tree)\n",
        "            \n",
        "            # Update the guess by adding the predictions of the new tree\n",
        "            guess += self.learning_rate * tree.predict(X)\n",
        "            \n",
        "            # Record the training loss\n",
        "            y_pred = self.predict(X)\n",
        "            loss = mse_loss(y, y_pred)\n",
        "            self.training_losses.append(loss)\n",
        "            \n",
        "    def predict(self, X):\n",
        "        # Start with the mean of the targets as our guess\n",
        "        y_pred = np.full((len(X),), np.mean(y))\n",
        "        \n",
        "        # Add the predictions of each tree with a scaled weight\n",
        "        for tree in self.trees:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            \n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "yUzzUWWAAfAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model = GradientBoostingRegressor(n_trees=100, max_depth=3, learning_rate=0.1)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print('MSE:', mean_squared_error(y, y_pred))\n",
        "print('R-squared:', r2_score(y, y_pred))"
      ],
      "metadata": {
        "id": "7artaXciD3W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best\n",
        "hyperparameters'''\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=4,\n",
        "                           n_informative=2, n_redundant=0,\n",
        "                          random_state=0, shuffle=False)\n",
        "parameters = {\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}"
      ],
      "metadata": {
        "id": "IAy1zJLGBdBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "AAsi3_v6GHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "classifier=GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "v5N1K0v_GKOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=classifier, param_grid=parameters, cv=5, n_jobs=-1,scoring=mean_squared_error)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "YT35AblbE1zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_search.best_params_\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "5mXy0e9dE8xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. What is a weak learner in Gradient Boosting?\n",
        "'''In Gradient Boosting algorithms, decision trees are used as the weak learner. \n",
        "Specifically, regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions. \n",
        "A weak learner is a model that is only slightly better than random guessing. \n",
        "Combined with many other weak learners, they can form a robust ensemble model to make accurate predictions.'''"
      ],
      "metadata": {
        "id": "_hryPV02_mIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "'''The intuition behind Gradient Boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. \n",
        "Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). \n",
        "Gradient boosting (GB) is based on the intuition that the next best possible model minimizes the overall prediction error when combined with previous models. \n",
        "The main principle is to set the target results for this next model to minimize the error2. In gradient boosting, a particular weak learner is trained on the dataset, and the errors or mistakes made by the algorithm are noted. \n",
        "Now while training a second weak learner algorithm, the errors and the mistakes that are made by the previous algorithm are passed into the second weak learner algorithm to avoid making the same mistake.'''"
      ],
      "metadata": {
        "id": "bo7zh-KT_0Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "'''Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to correct the errors made by existing models. \n",
        "The algorithm starts with a single model that makes predictions on the training dataset. \n",
        "The errors made by this model are then used to train a second model that attempts to correct these errors. \n",
        "This process is repeated until no further improvements can be made.'''"
      ],
      "metadata": {
        "id": "YeBgw-_RAFj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "'''The mathematical intuition of Gradient Boosting algorithm involves the following steps:\n",
        "1) Initialize the model with a constant value\n",
        "2) Compute the negative gradient of the loss function with respect to the model’s output\n",
        "3) Fit a weak learner to the negative gradient\n",
        "4) Update the model by adding the output of the weak learner\n",
        "5) Repeat steps 2-4 until convergence.'''"
      ],
      "metadata": {
        "id": "zo86WfjeAPQn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}