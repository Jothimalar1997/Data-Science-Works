{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ALZwdY-6NbQD",
        "outputId": "49fcbe1c-b8d5-4359-c4c0-3340189ee1ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables.\\nFor instance, when we predict rent based on square feet alone that is simple linear regression. When we predict rent based on square feet, number of bedrooms, and number of bathrooms that is multiple linear regression.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "'''Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables.\n",
        "For instance, when we predict rent based on square feet alone that is simple linear regression. When we predict rent based on square feet, number of bedrooms, and number of bathrooms that is multiple linear regression.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "'''The assumptions of linear regression are as follows:\n",
        "(i) Linearity: The relationship between the dependent variable and independent variable(s) should be linear.\n",
        "(ii) Independence: The observations should be independent of each other.\n",
        "(iii) Homoscedasticity: The variance of residuals should be constant across all levels of independent variables.\n",
        "(iv) Normality: The residuals should be normally distributed.\n",
        "(v) No multicollinearity: There should be no multicollinearity among independent variables.\n",
        "(i) To check whether these assumptions hold in a given dataset, you can use the following methods 123:\n",
        "(ii) Residual plot: A residual plot can help you identify non-linearity, unequal variances, and outliers.\n",
        "(iii) Normal probability plot: A normal probability plot can help you identify non-normality.\n",
        "(iv) Variance inflation factor (VIF): VIF can help you identify multicollinearity among independent variables.\n",
        "(v) Durbin-Watson test: Durbin-Watson test can help you identify autocorrelation among residuals.\n",
        "(vi) Cook’s distance: Cook’s distance can help you identify influential observations that may have a large impact on the regression model.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "swNHabaEPrum",
        "outputId": "add55424-7be6-4f84-e399-a5fd49f5eaff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The assumptions of linear regression are as follows:\\n(i) Linearity: The relationship between the dependent variable and independent variable(s) should be linear.\\n(ii) Independence: The observations should be independent of each other.\\n(iii) Homoscedasticity: The variance of residuals should be constant across all levels of independent variables.\\n(iv) Normality: The residuals should be normally distributed.\\n(v) No multicollinearity: There should be no multicollinearity among independent variables.\\n(i) To check whether these assumptions hold in a given dataset, you can use the following methods 123:\\n(ii) Residual plot: A residual plot can help you identify non-linearity, unequal variances, and outliers.\\n(iii) Normal probability plot: A normal probability plot can help you identify non-normality.\\n(iv) Variance inflation factor (VIF): VIF can help you identify multicollinearity among independent variables.\\n(v) Durbin-Watson test: Durbin-Watson test can help you identify autocorrelation among residuals.\\n(vi) Cook’s distance: Cook’s distance can help you identify influential observations that may have a large impact on the regression model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "'''In a linear regression model, the slope represents the change in the dependent variable for every one-unit change in the independent variable. \n",
        "The intercept represents the value of the dependent variable when the independent variable is zero.\n",
        "For example, let’s say you are a real estate agent and you want to predict the price of a house based on its size. \n",
        "You can use linear regression to model this relationship. In this case, the slope would represent how much the price of a house increases for every one-unit increase in size (e.g., square footage). \n",
        "The intercept would represent the price of a house when its size is zero.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qec0R0lxQe-p",
        "outputId": "003ddd1c-1988-41d3-abc9-135cd4f46e2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a linear regression model, the slope represents the change in the dependent variable for every one-unit change in the independent variable. \\nThe intercept represents the value of the dependent variable when the independent variable is zero.\\nFor example, let’s say you are a real estate agent and you want to predict the price of a house based on its size. \\nYou can use linear regression to model this relationship. In this case, the slope would represent how much the price of a house increases for every one-unit increase in size (e.g., square footage). \\nThe intercept would represent the price of a house when its size is zero.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "'''Gradient descent is an optimization algorithm mainly used to find the minimum of a function. \n",
        "In machine learning, gradient descent is used to update parameters in a model. Parameters can vary according to the algorithms, such as coefficients in Linear Regression and weights in Neural Networks.\n",
        "The algorithm works by iteratively adjusting the parameters of a model in the direction of steepest descent as defined by the negative of the gradient of the cost function. \n",
        "The gradient is calculated with respect to each parameter, and each parameter is updated by subtracting a fraction of the gradient from its current value.\n",
        "Gradient descent is used to update parameters in a model. Parameters can vary according to the algorithms, such as coefficients in Linear Regression and weights in Neural Networks1. \n",
        "It is by far the most popular optimization strategy used in machine learning and deep learning at the moment. It can be combined with every algorithm and is easy to understand and implement'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wQx9V32HQ0MS",
        "outputId": "09137b52-9d28-4e81-a403-216a28d02dfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gradient descent is an optimization algorithm mainly used to find the minimum of a function. \\nIn machine learning, gradient descent is used to update parameters in a model. Parameters can vary according to the algorithms, such as coefficients in Linear Regression and weights in Neural Networks.\\nThe algorithm works by iteratively adjusting the parameters of a model in the direction of steepest descent as defined by the negative of the gradient of the cost function. \\nThe gradient is calculated with respect to each parameter, and each parameter is updated by subtracting a fraction of the gradient from its current value.\\nGradient descent is used to update parameters in a model. Parameters can vary according to the algorithms, such as coefficients in Linear Regression and weights in Neural Networks1. \\nIt is by far the most popular optimization strategy used in machine learning and deep learning at the moment. It can be combined with every algorithm and is easy to understand and implement'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "'''Multiple linear regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. \n",
        "It is an extension of linear (OLS) regression that uses just one explanatory variable.\n",
        "In MLR, there are two or more independent variables used to predict the outcome of a dependent variable2. The formula for a multiple linear regression is:\n",
        "y = b0 + b1x1 + b2x2 + ... + bnxn\n",
        "where y is the predicted value of the dependent variable, b0 is the y-intercept (value of y when all other parameters are set to 0), and \n",
        "bn is the regression coefficient of the nth independent variable (a.k.a. the effect that increasing the value of the nth independent variable has on y).\n",
        "Difference between simple linear regression and mulitiple linear regression:\n",
        "(i) Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. \n",
        "(ii) In simple linear regression there is a one-to-one relationship between the input variable and the output variable. But in multiple linear regression, as the name implies there is a many-to-one relationship. \n",
        "In other words, multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gUDGuOdBRaGO",
        "outputId": "30a64791-e800-4636-c5a0-bd64429138d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multiple linear regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. \\nIt is an extension of linear (OLS) regression that uses just one explanatory variable.\\nIn MLR, there are two or more independent variables used to predict the outcome of a dependent variable2. The formula for a multiple linear regression is:\\ny = b0 + b1x1 + b2x2 + ... + bnxn\\nwhere y is the predicted value of the dependent variable, b0 is the y-intercept (value of y when all other parameters are set to 0), and \\nbn is the regression coefficient of the nth independent variable (a.k.a. the effect that increasing the value of the nth independent variable has on y).\\nDifference between simple linear regression and mulitiple linear regression:\\n(i) Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. \\n(ii) In simple linear regression there is a one-to-one relationship between the input variable and the output variable. But in multiple linear regression, as the name implies there is a many-to-one relationship. \\nIn other words, multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "'''Multicollinearity is a phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. \n",
        "This can lead to unstable and unreliable estimates of regression coefficients and can make it difficult to determine the effect of each independent variable on the dependent variable.\n",
        "The most common way to detect multicollinearity is by using the variance inflation factor (VIF), which measures the correlation and strength of correlation between the predictor variables in a regression model. \n",
        "A VIF value of 1 indicates no multicollinearity, while values greater than 1 indicate increasing levels of multicollinearity. A VIF value greater than 5 or 10 is generally considered high.\n",
        "There are several ways to address multicollinearity in a multiple linear regression model. \n",
        "One way is to remove one or more of the highly correlated independent variables from the model.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "pjWipZFwRaMo",
        "outputId": "32e61be4-817b-4efd-bf15-7e2bc04d8e83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multicollinearity is a phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. \\nThis can lead to unstable and unreliable estimates of regression coefficients and can make it difficult to determine the effect of each independent variable on the dependent variable.\\nThe most common way to detect multicollinearity is by using the variance inflation factor (VIF), which measures the correlation and strength of correlation between the predictor variables in a regression model. \\nA VIF value of 1 indicates no multicollinearity, while values greater than 1 indicate increasing levels of multicollinearity. A VIF value greater than 5 or 10 is generally considered high.\\nThere are several ways to address multicollinearity in a multiple linear regression model. \\nOne way is to remove one or more of the highly correlated independent variables from the model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "'''Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. \n",
        "It is used when the data points are not captured by a linear regression model.\n",
        "The main difference between linear and polynomial regression is that linear regression models a linear relationship between the dependent variable and independent variable whereas polynomial regression \n",
        "models a non-linear relationship between the dependent variable and independent variable.\n",
        "In other words, linear regression can only model linear relationships while polynomial regression can model both linear and non-linear relationships.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "izq7ACUQTREJ",
        "outputId": "87e922f5-eb91-4ae2-873b-0526beb22aa0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial. \\nIt is used when the data points are not captured by a linear regression model.\\nThe main difference between linear and polynomial regression is that linear regression models a linear relationship between the dependent variable and independent variable whereas polynomial regression \\nmodels a non-linear relationship between the dependent variable and independent variable.\\nIn other words, linear regression can only model linear relationships while polynomial regression can model both linear and non-linear relationships.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "'''Advantages of using Polynomial Regression:\n",
        "(i) It provides the best approximation of the relationship between the dependent and independent variable.\n",
        "(ii) A broad range of function can be fit under it.\n",
        "(iii) Polynomial basically fits a wide range of curvature.\n",
        "Disadvantages of using Polynomial Regression:\n",
        "(i) These are too sensitive to the outliers.\n",
        "(ii) The presence of one or two outliers in the data can seriously affect the results of a nonlinear analysis1.\n",
        "(iii) In general, polynomial regression is used when there is a non-linear relationship between the dependent and independent variables. \n",
        "Situations where you might want to use polynomial regression:\n",
        "(i) It is also used when more accuracy is required as compared to linear regression.\n",
        "(ii) It is generally used when the points in the data are not captured by the Linear Regression Model and the Linear Regression fails in describing the best result clearly.\n",
        "(iii) When you have a non-linear relationship between your independent and dependent variables.\n",
        "(iv) When you want to fit a curve to your data.\n",
        "(v) When you want to model a relationship between two variables that is not linear.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "RBODZIo5TmG4",
        "outputId": "f7e88550-e14d-40ad-b3bd-d2e15f193886"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Advantages of using Polynomial Regression:\\n(i) It provides the best approximation of the relationship between the dependent and independent variable.\\n(ii) A broad range of function can be fit under it.\\n(iii) Polynomial basically fits a wide range of curvature.\\nDisadvantages of using Polynomial Regression:\\n(i) These are too sensitive to the outliers.\\n(ii) The presence of one or two outliers in the data can seriously affect the results of a nonlinear analysis1.\\n(iii) In general, polynomial regression is used when there is a non-linear relationship between the dependent and independent variables. \\nSituations where you might want to use polynomial regression:\\n(i) It is also used when more accuracy is required as compared to linear regression.\\n(ii) It is generally used when the points in the data are not captured by the Linear Regression Model and the Linear Regression fails in describing the best result clearly.\\n(iii) When you have a non-linear relationship between your independent and dependent variables.\\n(iv) When you want to fit a curve to your data.\\n(v) When you want to model a relationship between two variables that is not linear.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}